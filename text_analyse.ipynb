{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_0VCwNKcCJ8"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import syllables\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import chardet\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hxa-TLmcScVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEuXSGzDtZ4w",
        "outputId": "70b156d5-40c6-404f-c4dc-434fbf1261f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_encoding(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        result = chardet.detect(file.read())\n",
        "    return result['encoding']"
      ],
      "metadata": {
        "id": "WYW3vt23XfNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/\"\n",
        "dir_list = os.listdir(path)\n",
        "newList=[]\n",
        "for filename in dir_list:\n",
        "  if \"StopWords\" in filename:\n",
        "    newList.append(filename)\n",
        "\n",
        "stops=''"
      ],
      "metadata": {
        "id": "26py7jvkZMoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in newList:\n",
        "  file_encoding = detect_encoding(file)\n",
        "  with open(file, 'r+', encoding=file_encoding) as content:\n",
        "    data1=content.read()\n",
        "    data1=data1.upper()\n",
        "    data1=data1.strip(' ')\n",
        "    stops+=data1"
      ],
      "metadata": {
        "id": "F2qTmsOzgEki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stops=stops.replace('\\n','|')\n",
        "stops=stops.split('|')\n",
        "for i in range(len(stops)):\n",
        "  stops[i]=stops[i].strip(' ')"
      ],
      "metadata": {
        "id": "1ut_R_GZcpBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_encoding_n = detect_encoding(\"/content/negative-words.txt\")\n",
        "with open(\"/content/negative-words.txt\", 'r+', encoding=file_encoding_n) as content_n:\n",
        "    negatives=content_n.read()\n",
        "    negatives=negatives.upper()\n",
        "negative_tokens=negatives.split('\\n')"
      ],
      "metadata": {
        "id": "JaH6NYkWfQsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_encoding_p = detect_encoding(\"/content/positive-words.txt\")\n",
        "with open(\"/content/positive-words.txt\", 'r+', encoding=file_encoding_p) as content_p:\n",
        "    positives=content_p.read()\n",
        "    positives=positives.upper()\n",
        "positive_tokens=positives.split('\\n')"
      ],
      "metadata": {
        "id": "HMXpGKcnfE3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/updated_input.xlsx')"
      ],
      "metadata": {
        "id": "IM-fxXUyVZvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = [\"'\", \".\", \",\", \"?\", \"/\",\"(\", \")\", \"#\", \"@\", \"!\", \"'\", \";\", \"-\",\":\",\"’\",'”',\"$\",'“',\"%\"]"
      ],
      "metadata": {
        "id": "J2ajVwO5edCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pronouns = ['I', 'WE', 'MY', 'OURS', 'US']"
      ],
      "metadata": {
        "id": "cMh0hJOJ65wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnalyseText:\n",
        "    def __init__(self, file_path, stops, punctuations, positive_tokens, negative_tokens, pronouns):\n",
        "        self.file_path = file_path\n",
        "        self.stops = stops\n",
        "        self.punctuations = punctuations\n",
        "        self.pronouns = pronouns\n",
        "        self.df = pd.read_excel(file_path)\n",
        "        self.positive_tokens = positive_tokens\n",
        "        self.negative_tokens = negative_tokens\n",
        "\n",
        "    def num_pronouns(self):\n",
        "        result = []\n",
        "        for i in range(len(self.df)):\n",
        "            res = 0\n",
        "            text = self.df.iloc[i]['text']\n",
        "            for word in self.return_wo_stopwords(text):\n",
        "                if word.upper() in self.pronouns:\n",
        "                    res += 1\n",
        "            result.append(res)\n",
        "        return result\n",
        "\n",
        "    def return_wo_stopwords(self, text):\n",
        "        words = self.return_tokens(text)\n",
        "        result = [i for i in words if i not in self.stops and i not in self.punctuations]\n",
        "        return result\n",
        "\n",
        "    def return_tokens(self, text):\n",
        "      words = []\n",
        "      if str(text) != 'nan':\n",
        "        words = word_tokenize(text)\n",
        "      return words\n",
        "\n",
        "    def return_sent_tokens(self, text):\n",
        "        lines = []\n",
        "        if str(text) != 'nan':\n",
        "          lines = sent_tokenize(text)\n",
        "        return lines\n",
        "\n",
        "    def num_words(self):\n",
        "        result = []\n",
        "        for i in range(len(self.df)):\n",
        "            text = self.df.iloc[i]['text']\n",
        "            words = self.return_wo_stopwords(text)\n",
        "            res = 0\n",
        "            for word in words:\n",
        "                if len(word) > 2 or (len(word) == 1 and word not in self.punctuations and word not in self.stops):\n",
        "                    res += 1\n",
        "            result.append(res)\n",
        "        return result\n",
        "\n",
        "    def num_sentences(self):\n",
        "        result = []\n",
        "        for i in range(len(self.df)):\n",
        "            text = self.df.iloc[i]['text']\n",
        "            sentences = self.return_sent_tokens(text)\n",
        "            result.append(len(sentences))\n",
        "        return result\n",
        "\n",
        "    # complex words\n",
        "    def analyse_readability(self):\n",
        "        fog = []\n",
        "        percent_complex = []\n",
        "        num_complex = []\n",
        "        avg_sent_len = self.avg_sent_len()\n",
        "        num_words = self.num_words()\n",
        "        for i in range(len(self.df)):\n",
        "            print(i)\n",
        "            words = self.return_tokens(self.df.iloc[i]['text'])\n",
        "            print(words)\n",
        "            print(fog)\n",
        "            if num_words[i] > 0:\n",
        "              fog.append(0.4 * (avg_sent_len[i] + (self.count_complex_words(words) / num_words[i])*100))\n",
        "              percent_complex.append(self.count_complex_words(words) / num_words[i])\n",
        "            else:\n",
        "              fog.append('')\n",
        "              percent_complex.append('')\n",
        "            num_complex.append(self.count_complex_words(words))\n",
        "        return fog, percent_complex, num_complex\n",
        "\n",
        "    def avg_sent_len(self):\n",
        "      num_words = self.num_words()\n",
        "      num_sentences = self.num_sentences()\n",
        "      return [num_words[i] / num_sentences[i] if num_sentences[i] > 0 else 0 for i in range(len(num_words))]\n",
        "\n",
        "    def avg_word_length(self):\n",
        "      result = []\n",
        "      num_words = self.num_words()\n",
        "      for i in range(len(self.df)):\n",
        "        words = self.return_wo_stopwords(self.df.iloc[i]['text'])\n",
        "        res = 0\n",
        "        for word in words:\n",
        "            if len(word) > 2 or (len(word) == 1 and word not in self.punctuations):\n",
        "              res += len(word)\n",
        "        if num_words[i] > 0:\n",
        "          print(res / num_words[i])\n",
        "          result.append(res / num_words[i])\n",
        "        else:\n",
        "          result.append('')\n",
        "      return result\n",
        "\n",
        "    def count_complex_words(self, words):\n",
        "        return sum(1 for word in words if syllables.estimate(word) > 2)\n",
        "\n",
        "    def positive_score(self):\n",
        "        result = []\n",
        "        for i in range(len(self.df)):\n",
        "            res = 0\n",
        "            text = self.df.iloc[i]['text']\n",
        "            words = self.return_tokens(text)\n",
        "            for word in words:\n",
        "                if word.upper() in self.positive_tokens:\n",
        "                    res += 1\n",
        "            result.append(res)\n",
        "        return result\n",
        "\n",
        "    def negative_score(self):\n",
        "        result = []\n",
        "        for i in range(len(self.df)):\n",
        "            res = 0\n",
        "            text = self.df.iloc[i]['text']\n",
        "            words = self.return_tokens(text)\n",
        "            for word in words:\n",
        "                if word.upper() in self.negative_tokens:\n",
        "                    res += 1\n",
        "            result.append(res)\n",
        "        return result\n",
        "\n",
        "    def num_syllables(self):\n",
        "        result = []\n",
        "        num_words = self.num_words()\n",
        "        for i in range(len(self.df)):\n",
        "          words = self.return_wo_stopwords(self.df.iloc[i]['text'])\n",
        "          res = 0\n",
        "          for word in words:\n",
        "            res += syllables.estimate(word)\n",
        "          if num_words[i] > 0:\n",
        "            result.append(res / num_words[i])\n",
        "          else:\n",
        "            result.append(0)\n",
        "        return result\n",
        "\n",
        "    def polarity_and_subjectivity(self):\n",
        "        pos = self.positive_score()\n",
        "        neg = self.negative_score()\n",
        "        words = self.num_words()\n",
        "        polarity = []\n",
        "        subjectivity = []\n",
        "        for i in range(len(self.df)):\n",
        "          polarity_value = (pos[i] - neg[i]) / (pos[i] + neg[i] + 0.000001)\n",
        "          subjectivity_value = (pos[i] + neg[i]) / (words[i] + 0.000001)\n",
        "          polarity.append(polarity_value)\n",
        "          subjectivity.append(subjectivity_value)\n",
        "\n",
        "        return polarity, subjectivity"
      ],
      "metadata": {
        "id": "Ddk8Bhc37f6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyser = AnalyseText('/content/updated_input.xlsx', stops, punctuations, positive_tokens, negative_tokens, pronouns)\n",
        "num_pronouns = analyser.num_pronouns()"
      ],
      "metadata": {
        "id": "Ro1m1rpOLzT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_score = analyser.positive_score()\n",
        "neg_score = analyser.negative_score()\n",
        "polarity, subjectivity = analyser.polarity_and_subjectivity()\n",
        "num_words = analyser.num_words()\n",
        "num_sentences = analyser.num_sentences()\n",
        "num_pronouns = analyser.num_pronouns()\n",
        "fog, percent_complex, count_complex = analyser.analyse_readability()\n",
        "syllables_per_word = analyser.num_syllables()\n",
        "avg_sent_len = analyser.avg_sent_len()\n",
        "avg_word_length = analyser.avg_word_length()"
      ],
      "metadata": {
        "id": "Ku1JLqrlgaCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['POSITIVE SCORE'] = pos_score\n",
        "df['NEGATIVE SCORE'] = neg_score\n",
        "df['POLARITY SCORE'] = polarity\n",
        "df['SUBJECTIVITY SCORE'] = subjectivity\n",
        "df['AVG SENTENCE LENGTH'] = avg_sent_len\n",
        "df['PERCENTAGE OF COMPLEX WORDS'] = percent_complex\n",
        "df['FOG INDEX'] = fog\n",
        "df['AVG NUMBER OF WORDS PER SENTENCE'] = avg_sent_len\n",
        "df['COMPLEX WORD COUNT'] = count_complex\n",
        "df['WORD COUNT'] = num_words\n",
        "df['SYLLABLE PER WORD'] = syllables_per_word\n",
        "df['PERSONAL PRONOUNS'] = num_pronouns\n",
        "df['AVERAGE WORD LENGTH'] = avg_word_length"
      ],
      "metadata": {
        "id": "VhBkbVNhC0pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('text', axis = 'columns')"
      ],
      "metadata": {
        "id": "bsl-ikGiNRih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('output.xlsx')"
      ],
      "metadata": {
        "id": "uIHuTj-5Nndk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}